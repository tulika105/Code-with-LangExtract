{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install LangExtract**"
      ],
      "metadata": {
        "id": "xK8gieb49R5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langextract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8JmgfTs9SGJ",
        "outputId": "94ad8e7f-7bbb-4ec1-a475-e6486f71d1a5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langextract\n",
            "  Downloading langextract-1.0.8-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.4.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (3.12.15)\n",
            "Collecting async_timeout>=4.0.0 (from langextract)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting exceptiongroup>=1.1.0 (from langextract)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-genai>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.30.0)\n",
            "Collecting ml-collections>=0.1.0 (from langextract)\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: more-itertools>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (10.7.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.2.2)\n",
            "Requirement already satisfied: pydantic>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.11.7)\n",
            "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (1.1.1)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from langextract) (4.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.0->langextract) (1.20.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (4.10.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai>=0.1.0->langextract) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->langextract) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.8.0->langextract) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->langextract) (2025.8.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai>=0.1.0->langextract) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai>=0.1.0->langextract) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai>=0.1.0->langextract) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->langextract) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai>=0.1.0->langextract) (0.6.1)\n",
            "Downloading langextract-1.0.8-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-collections, exceptiongroup, async_timeout, langextract\n",
            "Successfully installed async_timeout-5.0.1 exceptiongroup-1.3.0 langextract-1.0.8 ml-collections-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary packages**"
      ],
      "metadata": {
        "id": "gEop7W8MLu9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langextract as lx\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "0P-gw7mU9V9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extraction Steps**"
      ],
      "metadata": {
        "id": "_VFhmZxWLycO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the prompt and extraction rules\n",
        "prompt = textwrap.dedent(\"\"\"\\\n",
        "    Extract all phrases that express a sentiment.\n",
        "    Use the exact text for the extraction. Do not paraphrase.\n",
        "    For each sentiment, provide the following attributes:\n",
        "    - category: The type of sentiment (e.g., 'positive', 'negative', 'neutral').\n",
        "    - intensity: A score from 0.0 to 1.0 indicating the strength of the sentiment.\"\"\")"
      ],
      "metadata": {
        "id": "717anax19l9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Provide a high-quality example to guide the model\n",
        "examples = [\n",
        "    lx.data.ExampleData(\n",
        "        text= \"The setup process was incredibly intuitive and the performance is amazing.\",\n",
        "        extractions=[\n",
        "            lx.data.Extraction(\n",
        "                extraction_class= \"sentiment\",\n",
        "                extraction_text= \"incredibly intuitive\",\n",
        "                attributes={\"category\": \"positive\", \"intensity\": 0.7}\n",
        "            ),\n",
        "            lx.data.Extraction(\n",
        "                extraction_class= \"sentiment\",\n",
        "                extraction_text= \"amazing\",\n",
        "                attributes= {\"category\": \"positive\", \"intensity\": 0.9}\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "RzSDsYDP-Sos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Prepare the input text for analysis and run the extraction\n",
        "input_text = \"\"\"\n",
        "The city air was thick with the smell of rain and exhaust as Kaelen clutched the faded photograph.\n",
        "A wave of profound sadness washed over him as he remembered happier times.\n",
        "He had to find her. Across the street, a shadowy figure watched from an alley, a flicker of cold amusement in their eyes before they melted back into the darkness.\n",
        "\"\"\"\n",
        "result = lx.extract(\n",
        "    text_or_documents=input_text,\n",
        "    prompt_description= prompt,\n",
        "    examples= examples,\n",
        "    model_id=\"gemini-2.5-flash\",\n",
        "    api_key=\"AIzaSyDYrdzbGWffZasw9PFq41f0o565t0xEwaY\"  # Only use this for testing\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4b0EQQN-wNU",
        "outputId": "593b8569-2744-435c-eaa1-313061316cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:absl:Registered GeminiLanguageModel with patterns ['^gemini'] at priority 10\n",
            "DEBUG:absl:Registered OllamaLanguageModel with patterns ['^gemma', '^llama', '^mistral', '^mixtral', '^phi', '^qwen', '^deepseek', '^command-r', '^starcoder', '^codellama', '^codegemma', '^tinyllama', '^wizardcoder', '^gpt-oss', '^meta-llama/[Ll]lama', '^google/gemma', '^mistralai/[Mm]istral', '^mistralai/[Mm]ixtral', '^microsoft/phi', '^Qwen/', '^deepseek-ai/', '^bigcode/starcoder', '^codellama/', '^TinyLlama/', '^WizardLM/'] at priority 10\n",
            "DEBUG:absl:Registered OpenAILanguageModel with patterns ['^gpt-4', '^gpt4\\\\.', '^gpt-5', '^gpt5\\\\.'] at priority 10\n",
            "2025-08-21 19:03:50,680 - langextract.debug - DEBUG - [langextract.inference] CALL: BaseLanguageModel.__init__(self=<GeminiLanguageModel>, constraint=Constraint(co...NONE: 'none'>), kwargs={})\n",
            "2025-08-21 19:03:50,681 - langextract.debug - DEBUG - [langextract.inference] RETURN: BaseLanguageModel.__init__ -> None (0.0 ms)\n",
            "2025-08-21 19:03:50,683 - langextract.debug - DEBUG - [langextract.inference] CALL: BaseLanguageModel.apply_schema(self=<GeminiLanguageModel>, schema_instance=GeminiSchema(...xtractions']}))\n",
            "2025-08-21 19:03:50,684 - langextract.debug - DEBUG - [langextract.inference] RETURN: BaseLanguageModel.apply_schema -> None (0.0 ms)\n",
            "DEBUG:absl:Initialized Annotator with prompt:\n",
            "Extract all phrases that express a sentiment.\n",
            "Use the exact text for the extraction. Do not paraphrase.\n",
            "For each sentiment, provide the following attributes:\n",
            "- category: The type of sentiment (e.g., 'positive', 'negative', 'neutral').\n",
            "- intensity: A score from 0.0 to 1.0 indicating the strength of the sentiment.\n",
            "\n",
            "Examples\n",
            "Q: The setup process was incredibly intuitive and the performance is amazing.\n",
            "A: {\n",
            "  \"extractions\": [\n",
            "    {\n",
            "      \"sentiment\": \"incredibly intuitive\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"positive\",\n",
            "        \"intensity\": 0.7\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"sentiment\": \"amazing\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"positive\",\n",
            "        \"intensity\": 0.9\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Q: \n",
            "A: \n",
            "INFO:absl:Starting document annotation.\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m [00:00]2025-08-21 19:03:50,688 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='\\nThe city air was thick with the smell of rain and exhaust as Kaelen clutched the faded photograph. \\nA wave of profound sadness washed over him as he remembered happier times. \\nHe had to find her. Across the street, a shadowy figure watched from an alley, a flicker of cold amusement in their eyes before they melted back into the darkness.\\n')\n",
            "2025-08-21 19:03:50,689 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.5 ms)\n",
            "INFO:absl:Processing batch 0 with length 1\n",
            "DEBUG:absl:Token util returns string: The city air was thick with the smell of rain and exhaust as Kaelen clutched the faded photograph. \n",
            "A wave of profound sadness washed over him as he remembered happier times. \n",
            "He had to find her. Across the street, a shadowy figure watched from an alley, a flicker of cold amusement in their eyes before they melted back into the darkness. for tokenized_text: TokenizedText(text='\\nThe city air was thick with the smell of rain and exhaust as Kaelen clutched the faded photograph. \\nA wave of profound sadness washed over him as he remembered happier times. \\nHe had to find her. Across the street, a shadowy figure watched from an alley, a flicker of cold amusement in their eyes before they melted back into the darkness.\\n', tokens=[Token(index=0, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=1, end_pos=4), first_token_after_newline=False), Token(index=1, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=5, end_pos=9), first_token_after_newline=False), Token(index=2, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=10, end_pos=13), first_token_after_newline=False), Token(index=3, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=14, end_pos=17), first_token_after_newline=False), Token(index=4, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=18, end_pos=23), first_token_after_newline=False), Token(index=5, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=24, end_pos=28), first_token_after_newline=False), Token(index=6, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=29, end_pos=32), first_token_after_newline=False), Token(index=7, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=33, end_pos=38), first_token_after_newline=False), Token(index=8, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=39, end_pos=41), first_token_after_newline=False), Token(index=9, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=42, end_pos=46), first_token_after_newline=False), Token(index=10, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=47, end_pos=50), first_token_after_newline=False), Token(index=11, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=51, end_pos=58), first_token_after_newline=False), Token(index=12, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=59, end_pos=61), first_token_after_newline=False), Token(index=13, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=62, end_pos=68), first_token_after_newline=False), Token(index=14, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=69, end_pos=77), first_token_after_newline=False), Token(index=15, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=78, end_pos=81), first_token_after_newline=False), Token(index=16, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=82, end_pos=87), first_token_after_newline=False), Token(index=17, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=88, end_pos=98), first_token_after_newline=False), Token(index=18, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=98, end_pos=99), first_token_after_newline=False), Token(index=19, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=101, end_pos=102), first_token_after_newline=True), Token(index=20, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=103, end_pos=107), first_token_after_newline=False), Token(index=21, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=108, end_pos=110), first_token_after_newline=False), Token(index=22, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=111, end_pos=119), first_token_after_newline=False), Token(index=23, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=120, end_pos=127), first_token_after_newline=False), Token(index=24, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=128, end_pos=134), first_token_after_newline=False), Token(index=25, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=135, end_pos=139), first_token_after_newline=False), Token(index=26, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=140, end_pos=143), first_token_after_newline=False), Token(index=27, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=144, end_pos=146), first_token_after_newline=False), Token(index=28, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=147, end_pos=149), first_token_after_newline=False), Token(index=29, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=150, end_pos=160), first_token_after_newline=False), Token(index=30, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=161, end_pos=168), first_token_after_newline=False), Token(index=31, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=169, end_pos=174), first_token_after_newline=False), Token(index=32, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=174, end_pos=175), first_token_after_newline=False), Token(index=33, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=177, end_pos=179), first_token_after_newline=True), Token(index=34, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=180, end_pos=183), first_token_after_newline=False), Token(index=35, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=184, end_pos=186), first_token_after_newline=False), Token(index=36, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=187, end_pos=191), first_token_after_newline=False), Token(index=37, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=192, end_pos=195), first_token_after_newline=False), Token(index=38, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=195, end_pos=196), first_token_after_newline=False), Token(index=39, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=197, end_pos=203), first_token_after_newline=False), Token(index=40, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=204, end_pos=207), first_token_after_newline=False), Token(index=41, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=208, end_pos=214), first_token_after_newline=False), Token(index=42, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=214, end_pos=215), first_token_after_newline=False), Token(index=43, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=216, end_pos=217), first_token_after_newline=False), Token(index=44, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=218, end_pos=225), first_token_after_newline=False), Token(index=45, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=226, end_pos=232), first_token_after_newline=False), Token(index=46, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=233, end_pos=240), first_token_after_newline=False), Token(index=47, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=241, end_pos=245), first_token_after_newline=False), Token(index=48, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=246, end_pos=248), first_token_after_newline=False), Token(index=49, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=249, end_pos=254), first_token_after_newline=False), Token(index=50, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=254, end_pos=255), first_token_after_newline=False), Token(index=51, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=256, end_pos=257), first_token_after_newline=False), Token(index=52, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=258, end_pos=265), first_token_after_newline=False), Token(index=53, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=266, end_pos=268), first_token_after_newline=False), Token(index=54, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=269, end_pos=273), first_token_after_newline=False), Token(index=55, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=274, end_pos=283), first_token_after_newline=False), Token(index=56, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=284, end_pos=286), first_token_after_newline=False), Token(index=57, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=287, end_pos=292), first_token_after_newline=False), Token(index=58, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=293, end_pos=297), first_token_after_newline=False), Token(index=59, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=298, end_pos=304), first_token_after_newline=False), Token(index=60, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=305, end_pos=309), first_token_after_newline=False), Token(index=61, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=310, end_pos=316), first_token_after_newline=False), Token(index=62, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=317, end_pos=321), first_token_after_newline=False), Token(index=63, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=322, end_pos=326), first_token_after_newline=False), Token(index=64, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=327, end_pos=330), first_token_after_newline=False), Token(index=65, token_type=<TokenType.WORD: 0>, char_interval=CharInterval(start_pos=331, end_pos=339), first_token_after_newline=False), Token(index=66, token_type=<TokenType.PUNCTUATION: 2>, char_interval=CharInterval(start_pos=339, end_pos=340), first_token_after_newline=False)]), token_interval: TokenInterval(start_index=0, end_index=67)\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m, current=\u001b[92m339\u001b[0m chars, processed=\u001b[92m339\u001b[0m chars:  [00:00]DEBUG:absl:Processing chunk: TextChunk(\n",
            "  interval=[start_index: 0, end_index: 67],\n",
            "  Document ID: doc_73bb4009,\n",
            "  Chunk Text: 'The city air was thick with the smell of rain and exhaust as Kaelen clutched the faded photograph. \n",
            "A wave of profound sadness washed over him as he remembered happier times. \n",
            "He had to find her. Across the street, a shadowy figure watched from an alley, a flicker of cold amusement in their eyes before they melted back into the darkness.'\n",
            ")\n",
            "DEBUG:absl:Top inference result: {\n",
            "  \"extractions\": [\n",
            "    {\n",
            "      \"sentiment\": \"profound sadness\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"negative\",\n",
            "        \"intensity\": \n",
            "        \"0.9\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"sentiment\": \"happier times\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"positive\",\n",
            "        \"intensity\": \n",
            "        \"0.7\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"sentiment\": \"cold amusement\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"negative\",\n",
            "        \"intensity\": \n",
            "        \"0.7\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "INFO:absl:Starting resolver process for input text.\n",
            "DEBUG:absl:Input Text: {\n",
            "  \"extractions\": [\n",
            "    {\n",
            "      \"sentiment\": \"profound sadness\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"negative\",\n",
            "        \"intensity\": \n",
            "        \"0.9\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"sentiment\": \"happier times\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"positive\",\n",
            "        \"intensity\": \n",
            "        \"0.7\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"sentiment\": \"cold amusement\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"negative\",\n",
            "        \"intensity\": \n",
            "        \"0.7\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "INFO:absl:Starting string parsing.\n",
            "DEBUG:absl:input_string: {\n",
            "  \"extractions\": [\n",
            "    {\n",
            "      \"sentiment\": \"profound sadness\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"negative\",\n",
            "        \"intensity\": \n",
            "        \"0.9\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"sentiment\": \"happier times\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"positive\",\n",
            "        \"intensity\": \n",
            "        \"0.7\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"sentiment\": \"cold amusement\",\n",
            "      \"sentiment_attributes\": {\n",
            "        \"category\": \"negative\",\n",
            "        \"intensity\": \n",
            "        \"0.7\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "DEBUG:absl:Successfully parsed content.\n",
            "INFO:absl:Completed parsing of string.\n",
            "DEBUG:absl:Parsed content: [{'sentiment': 'profound sadness', 'sentiment_attributes': {'category': 'negative', 'intensity': '0.9'}}, {'sentiment': 'happier times', 'sentiment_attributes': {'category': 'positive', 'intensity': '0.7'}}, {'sentiment': 'cold amusement', 'sentiment_attributes': {'category': 'negative', 'intensity': '0.7'}}]\n",
            "INFO:absl:Starting to extract and order extractions from data.\n",
            "INFO:absl:Completed extraction and ordering of extractions.\n",
            "DEBUG:absl:Completed the resolver process.\n",
            "INFO:absl:Starting alignment process for provided chunk text.\n",
            "DEBUG:absl:WordAligner: Starting alignment of extractions with the source text. Extraction groups to align: [[Extraction(extraction_class='sentiment', extraction_text='profound sadness', char_interval=None, alignment_status=None, extraction_index=1, group_index=0, description=None, attributes={'category': 'negative', 'intensity': '0.9'}), Extraction(extraction_class='sentiment', extraction_text='happier times', char_interval=None, alignment_status=None, extraction_index=2, group_index=1, description=None, attributes={'category': 'positive', 'intensity': '0.7'}), Extraction(extraction_class='sentiment', extraction_text='cold amusement', char_interval=None, alignment_status=None, extraction_index=3, group_index=2, description=None, attributes={'category': 'negative', 'intensity': '0.7'})]]\n",
            "2025-08-21 19:03:55,184 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='The city air was thick with the smell of rain and exhaust as Kaelen clutched the faded photograph. \\nA wave of profound sadness washed over him as he remembered happier times. \\nHe had to find her. Across the street, a shadowy figure watched from an alley, a flicker of cold amusement in their eyes before they melted back into the darkness.')\n",
            "2025-08-21 19:03:55,185 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.3 ms)\n",
            "2025-08-21 19:03:55,186 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='␟')\n",
            "2025-08-21 19:03:55,188 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "DEBUG:absl:Using delimiter '␟' for extraction alignment\n",
            "2025-08-21 19:03:55,189 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='profound sadness ␟ happier times ␟ cold amusement')\n",
            "2025-08-21 19:03:55,190 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.1 ms)\n",
            "DEBUG:absl:Processing extraction group 0 with 3 extractions.\n",
            "2025-08-21 19:03:55,191 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='profound sadness')\n",
            "2025-08-21 19:03:55,191 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-21 19:03:55,192 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='happier times')\n",
            "2025-08-21 19:03:55,192 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-21 19:03:55,193 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='cold amusement')\n",
            "2025-08-21 19:03:55,193 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-21 19:03:55,194 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='The city air was thick with the smell of rain and exhaust as Kaelen clutched the faded photograph. \\nA wave of profound sadness washed over him as he remembered happier times. \\nHe had to find her. Across the street, a shadowy figure watched from an alley, a flicker of cold amusement in their eyes before they melted back into the darkness.')\n",
            "2025-08-21 19:03:55,195 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.3 ms)\n",
            "2025-08-21 19:03:55,195 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='profound sadness')\n",
            "2025-08-21 19:03:55,196 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-21 19:03:55,196 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='happier times')\n",
            "2025-08-21 19:03:55,197 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "2025-08-21 19:03:55,197 - langextract.debug - DEBUG - [langextract.tokenizer] CALL: tokenize(text='cold amusement')\n",
            "2025-08-21 19:03:55,198 - langextract.debug - DEBUG - [langextract.tokenizer] RETURN: tokenize -> TokenizedText...wline=False)]) (0.0 ms)\n",
            "DEBUG:absl:Final aligned extraction groups: [[Extraction(extraction_class='sentiment', extraction_text='profound sadness', char_interval=CharInterval(start_pos=111, end_pos=127), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=1, group_index=0, description=None, attributes={'category': 'negative', 'intensity': '0.9'}), Extraction(extraction_class='sentiment', extraction_text='happier times', char_interval=CharInterval(start_pos=161, end_pos=174), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=2, group_index=1, description=None, attributes={'category': 'positive', 'intensity': '0.7'}), Extraction(extraction_class='sentiment', extraction_text='cold amusement', char_interval=CharInterval(start_pos=269, end_pos=283), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=3, group_index=2, description=None, attributes={'category': 'negative', 'intensity': '0.7'})]]\n",
            "DEBUG:absl:Aligned extractions count: 3\n",
            "DEBUG:absl:Yielding aligned extraction: Extraction(extraction_class='sentiment', extraction_text='profound sadness', char_interval=CharInterval(start_pos=111, end_pos=127), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=1, group_index=0, description=None, attributes={'category': 'negative', 'intensity': '0.9'})\n",
            "DEBUG:absl:Yielding aligned extraction: Extraction(extraction_class='sentiment', extraction_text='happier times', char_interval=CharInterval(start_pos=161, end_pos=174), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=2, group_index=1, description=None, attributes={'category': 'positive', 'intensity': '0.7'})\n",
            "DEBUG:absl:Yielding aligned extraction: Extraction(extraction_class='sentiment', extraction_text='cold amusement', char_interval=CharInterval(start_pos=269, end_pos=283), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=3, group_index=2, description=None, attributes={'category': 'negative', 'intensity': '0.7'})\n",
            "INFO:absl:Completed alignment process for the provided source_text.\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mgemini-2.5-flash\u001b[0m, current=\u001b[92m339\u001b[0m chars, processed=\u001b[92m339\u001b[0m chars:  [00:04]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Extraction processing complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "INFO:absl:Finalizing annotation for document ID doc_73bb4009.\n",
            "INFO:absl:Document annotation completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Extracted \u001b[1m3\u001b[0m entities (\u001b[1m1\u001b[0m unique types)\n",
            "  \u001b[96m•\u001b[0m Time: \u001b[1m4.52s\u001b[0m\n",
            "  \u001b[96m•\u001b[0m Speed: \u001b[1m75\u001b[0m chars/sec\n",
            "  \u001b[96m•\u001b[0m Chunks: \u001b[1m1\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Save the results to a JSONL file\n",
        "lx.io.save_annotated_documents([result], output_name=\"extraction_results.jsonl\", output_dir=\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5zcCmzd_ktC",
        "outputId": "111abe4a-af96-4d63-f964-4d5ec2a73fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92mextraction_results.jsonl\u001b[0m: 1 docs [00:00, 1568.55 docs/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92mextraction_results.jsonl\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Generate the visualization from the file\n",
        "html_content = lx.visualize(\"extraction_results.jsonl\")\n",
        "with open(\"visualization.html\", \"w\") as f:\n",
        "    if hasattr(html_content, 'data'):\n",
        "        f.write(html_content.data)  # For Colab\n",
        "    else:\n",
        "        f.write(html_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrZMQEmYBKIA",
        "outputId": "5b57fd89-916d-470a-d8a4-e9e0522a007e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Loading \u001b[92mextraction_results.jsonl\u001b[0m: 100%|██████████| 1.25k/1.25k [00:00<00:00, 2.99MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Loaded \u001b[1m1\u001b[0m documents from \u001b[92mextraction_results.jsonl\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}